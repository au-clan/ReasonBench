{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d2985e",
   "metadata": {},
   "source": [
    "# Notebook: Parsing\n",
    "\n",
    "Just to parse the logs and create csv files etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33de048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils import get_logs, save_dataframe, get_calls\n",
    "\n",
    "i = 1\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c77a9",
   "metadata": {},
   "source": [
    "## 1. Reasoning models\n",
    "Parsing the results for the reasoning model experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2b487",
   "metadata": {},
   "source": [
    "### 1.0 Namespace + Loading the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05159098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models we've run experiments on\n",
    "models = [\n",
    "    \"Qwen3-235B-A22B-Thinking-2507\",\n",
    "    \"gpt-oss-120b\", \n",
    "    \"DeepSeek-R1\", \n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\",     \n",
    "]\n",
    "\n",
    "# Experiments we've set up\n",
    "\"\"\"\n",
    "    - simple: Standard evaluation without any special prompting strategies\n",
    "    - repeats: Same as simple, but the experiment is repeated multiple times\n",
    "\"\"\"\n",
    "experiments = [\n",
    "    \"simple\",\n",
    "    \"repeats\"\n",
    "]\n",
    "\n",
    "# Benchmarks we've run experiments on\n",
    "benchmarks = [\n",
    "        \"game24\",\n",
    "        \"hle\",\n",
    "        \"hotpotqa\",\n",
    "        \"humaneval\",\n",
    "        \"matharena\",\n",
    "        \"scibench\",\n",
    "        \"sonnetwriting\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# Setup\n",
    "model = models[i]           # Model for which we want to load logs\n",
    "experiment = experiments[1] # Experiment for which we want to load the logs\n",
    "\n",
    "# Load the logs\n",
    "logs = get_logs(\n",
    "    logs_path=\"../logs\",\n",
    "    experiment=experiment,\n",
    "    model=model,\n",
    "    benchmarks=benchmarks,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f5fb8c",
   "metadata": {},
   "source": [
    "### 1.1 Parsing the logs to load the important information we'd like to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135e67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each entry in data corresponds to a single experiment\n",
    "data = []\n",
    "\n",
    "for log in logs:\n",
    "    \"\"\"\n",
    "        - log: dict --> The log for a single experiment\n",
    "        - temp: dict --> Temporary dictionary with the extracted/processed log data\n",
    "    \"\"\"\n",
    "    temp = {}\n",
    "    \n",
    "    try:\n",
    "        # Metadata \n",
    "        temp.update(log[\"General information\"])\n",
    "        #temp.update(log[\"Method Configuration\"])\n",
    "        temp.update(log[\"LLM Information\"])\n",
    "        temp[\"log\"] = log[\"Log path\"]\n",
    "\n",
    "        # Cost per sample\n",
    "        temp[\"costs\"] = [tab[\"Cost (total)\"][\"total\"] for tab in log[\"API Detailed Information (per tab)\"].values()]\n",
    "        temp[\"costs_in\"] = [tab[\"Cost (total)\"][\"in\"] for tab in log[\"API Detailed Information (per tab)\"].values()]\n",
    "        temp[\"costs_out\"] = [tab[\"Cost (total)\"][\"out\"] for tab in log[\"API Detailed Information (per tab)\"].values()]\n",
    "\n",
    "        # Tokens per sample\n",
    "        temp[\"tokens_in\"] = [tab[\"Tokens (total)\"][\"in\"] for tab in log[\"API Detailed Information (per tab)\"].values()]\n",
    "        temp[\"tokens_out\"] = [tab[\"Tokens (total)\"][\"out\"] for tab in log[\"API Detailed Information (per tab)\"].values()]\n",
    "\n",
    "        # Quality per sample\n",
    "        temp[\"scores\"] = log[\"Quality\"][\"Correct\"]\n",
    "        \n",
    "        data.append(temp)\n",
    "    except KeyError as e:\n",
    "        print(f\"Skipping log due to missing key: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed357ee",
   "metadata": {},
   "source": [
    "### 1.2 Loading the data in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3c535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into DataFrame. Each row corresponds to a single experiment\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame for future reference\n",
    "save_dataframe(\n",
    "    df=df,\n",
    "    data_path=\"../data/models\",\n",
    "    experiment=experiment,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Loading the data back (example)\n",
    "df = pd.read_parquet(\"../data/examples/models.parquet\")\n",
    "if verbose:\n",
    "    display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac9ae8",
   "metadata": {},
   "source": [
    "## 2. Reasoning strategies\n",
    "Parsing the results for the reasoning strategy experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662bb4f1",
   "metadata": {},
   "source": [
    "### 2.0 Namespace + Loading the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc94592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models we've run experiments on\n",
    "models = [\n",
    "    \"gpt-4.1-nano\",\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-5-nano\", \n",
    "    \"llama-4-scout-17b-16e-instruct\",\n",
    "    \"qwen3-32b\",\n",
    "]\n",
    "\n",
    "# Experiments we've set up\n",
    "\"\"\"\n",
    "    - simple: Standard evaluation without any special prompting strategies\n",
    "    - repeats: Same as simple, but the experiment is repeated multiple times\n",
    "\"\"\"\n",
    "experiments = [\n",
    "    \"simple\",\n",
    "    \"repeats\"\n",
    "]\n",
    "\n",
    "# Benchmarks we've run experiments on\n",
    "benchmarks = [\n",
    "        \"game24\",\n",
    "        \"hle\",\n",
    "        \"hotpotqa\",\n",
    "        \"humaneval\",\n",
    "        \"matharena\",\n",
    "        \"scibench\",\n",
    "        \"sonnetwriting\"\n",
    "    ]\n",
    "\n",
    "# Reasoning strategies we've employed\n",
    "strategies = [\n",
    "    \"io\",\n",
    "    \"cot\",\n",
    "    \"cot_sc\",\n",
    "    \"foa\",\n",
    "    \"tot_bfs\",\n",
    "    \"tot_dfs\",\n",
    "    \"got\",\n",
    "    \"react\",\n",
    "    \"reflection\",\n",
    "    \"rap\",\n",
    "    \"mcts\"\n",
    "]\n",
    "\n",
    "\n",
    "# Setup\n",
    "model = models[i]           # Model for which we want to load logs\n",
    "experiment = experiments[1] # Experiment for which we want to load the logs\n",
    "\n",
    "# Load the logs\n",
    "logs = get_logs(\n",
    "    logs_path=\"../logs\",\n",
    "    experiment=experiment,\n",
    "    model=model,\n",
    "    benchmarks=benchmarks,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b2bb6",
   "metadata": {},
   "source": [
    "### 2.1 Parsing the logs to load the important information we'd like to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28595d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each entry in data corresponds to a single experiment\n",
    "data = []\n",
    "\n",
    "for log in logs:\n",
    "    \"\"\"\n",
    "        - log: dict --> The log for a single experiment\n",
    "        - temp: dict --> Temporary dictionary with the extracted/processed log data\n",
    "    \"\"\"\n",
    "    temp = {}\n",
    "    \n",
    "    try:\n",
    "        # Metadata \n",
    "        temp.update(log[\"General information\"])\n",
    "        #temp.update(log[\"Method Configuration\"])\n",
    "        temp.update(log[\"LLM Information\"])\n",
    "        temp[\"log\"] = log[\"Log path\"]\n",
    "\n",
    "        # Cost per sample\n",
    "        temp[\"costs\"] = [tab[\"Cost (total)\"][\"total\"] for tab in log[\"API Detailed Information (per tab)\"].values()]\n",
    "        temp[\"costs_in\"] = [tab[\"Cost (total)\"][\"in\"] for tab in log[\"API Detailed Information (per tab)\"].values()]\n",
    "        temp[\"costs_out\"] = [tab[\"Cost (total)\"][\"out\"] for tab in log[\"API Detailed Information (per tab)\"].values()]\n",
    "\n",
    "        # Tokens per sample\n",
    "        temp[\"tokens_in\"] = [tab[\"Tokens (total)\"][\"in\"] for tab in log[\"API Detailed Information (per tab)\"].values()]\n",
    "        temp[\"tokens_out\"] = [tab[\"Tokens (total)\"][\"out\"] for tab in log[\"API Detailed Information (per tab)\"].values()]\n",
    "\n",
    "        # Quality per sample\n",
    "        temp[\"scores\"] = log[\"Quality\"][\"Correct\"]\n",
    "        \n",
    "        data.append(temp)\n",
    "    except KeyError as e:\n",
    "        print(f\"Skipping log due to missing key: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e40cc2",
   "metadata": {},
   "source": [
    "### 2.2 Loading the data in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3021809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into DataFrame. Each row corresponds to a single experiment\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame for future reference\n",
    "save_dataframe(\n",
    "    df=df,\n",
    "    data_path=\"../data/strategies\",\n",
    "    experiment=experiment,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Loading the data back (example)\n",
    "df = pd.read_parquet(\"../data/examples/models.parquet\")\n",
    "if verbose:\n",
    "    display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994d56a",
   "metadata": {},
   "source": [
    "## 3. Traces\n",
    "Parsing the actual reasoning traces. Mostly used for reasoning models but they can parse the respective logs of reasoning strategies as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c16e15",
   "metadata": {},
   "source": [
    "### 3.0 Namespace + Loading the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models we've run experiments on\n",
    "models = [\n",
    "    \"Qwen3-235B-A22B-Thinking-2507\",\n",
    "    \"gpt-oss-120b\", \n",
    "    \"DeepSeek-R1\", \n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\",     \n",
    "]\n",
    "\n",
    "# Experiments we've set up\n",
    "\"\"\"\n",
    "    - simple: Standard evaluation without any special prompting strategies\n",
    "    - repeats: Same as simple, but the experiment is repeated multiple times\n",
    "\"\"\"\n",
    "experiments = [\n",
    "    \"simple\",\n",
    "    \"repeats\"\n",
    "]\n",
    "\n",
    "# Benchmarks we've run experiments on\n",
    "benchmarks = [\n",
    "        \"game24\",\n",
    "        \"hle\",\n",
    "        \"hotpotqa\",\n",
    "        \"humaneval\",\n",
    "        \"matharena\",\n",
    "        \"scibench\",\n",
    "        \"sonnetwriting\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# Setup\n",
    "model = models[0]           # Model for which we want to load logs\n",
    "experiment = experiments[1] # Experiment for which we want to load the logs\n",
    "\n",
    "calls = get_calls(\n",
    "    logs_path=\"../logs\",\n",
    "    experiment=experiment,\n",
    "    model=model,\n",
    "    benchmarks=benchmarks,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7130202",
   "metadata": {},
   "source": [
    "### 3.1 Parsing the logs + metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc68fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each entry in data corresponds to a single experiment\n",
    "data = []\n",
    "\n",
    "for log in calls:\n",
    "    for call in log[\"calls\"]:\n",
    "        for response in call[\"responses\"]:\n",
    "            \"\"\"\n",
    "                - log: dict --> The log for a single experiment (keys are: benchmark, calls, path)\n",
    "                - call: dict --> The call information\n",
    "                - response: str --> One of the response samples of the call\n",
    "                - temp: dict --> Temporary dictionary with the extracted/processed log data\n",
    "            \"\"\"\n",
    "            temp = {\n",
    "                \"benchmark\": log[\"benchmark\"],\n",
    "                \"user\": call[\"user_message\"],\n",
    "                \"response\": response,\n",
    "                \"path\": log[\"path\"],\n",
    "            }\n",
    "            data.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939c508",
   "metadata": {},
   "source": [
    "### 3.2 Loading the data in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a02a0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into DataFrame. Each row corresponds to a single response sample\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame for future reference\n",
    "save_dataframe(\n",
    "    df=df,\n",
    "    data_path=\"../data/calls\",\n",
    "    experiment=experiment,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "df = pd.read_parquet(\"../data/examples/calls.parquet\")\n",
    "if verbose:\n",
    "    display(df.head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CacheSaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
